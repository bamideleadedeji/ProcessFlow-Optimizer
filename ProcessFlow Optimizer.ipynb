{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "701da2ef-64b6-49ba-b5f3-743277784cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for files in: C:\\Users\\user\\Portfolio_Exports\n",
      "Files found in Portfolio_Exports:\n",
      "  - Cleaned_BPI_2019_Events.csv (412.20 MB)\n",
      "  - Process_Bottleneck_Analysis.csv (0.00 MB)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Define the path\n",
    "base_path = Path.home() / \"Portfolio_Exports\"\n",
    "print(f\"Looking for files in: {base_path}\")\n",
    "\n",
    "# List available files\n",
    "if base_path.exists():\n",
    "    files = list(base_path.glob(\"*.csv\")) + list(base_path.glob(\"*.xlsx\")) + list(base_path.glob(\"*.parquet\"))\n",
    "    print(\"Files found in Portfolio_Exports:\")\n",
    "    for file in files:\n",
    "        print(f\"  - {file.name} ({file.stat().st_size / 1024**2:.2f} MB)\")\n",
    "else:\n",
    "    print(f\"Directory not found: {base_path}\")\n",
    "    # Try alternative paths\n",
    "    alt_paths = [\n",
    "        Path.home() / \"Portfolio_Export\",\n",
    "        Path.home() / \"Desktop\" / \"Portfolio_Exports\",\n",
    "        Path.home() / \"Documents\" / \"Portfolio_Exports\"\n",
    "    ]\n",
    "    for alt_path in alt_paths:\n",
    "        if alt_path.exists():\n",
    "            base_path = alt_path\n",
    "            files = list(base_path.glob(\"*\"))\n",
    "            print(f\"\\nFound in alternative location: {base_path}\")\n",
    "            for file in files[:10]:  # Show first 10 files\n",
    "                print(f\"  - {file.name}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0c6fd1b-6aa6-4f27-bf36-ecc54a3fc727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING BOTH DATASETS\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "LOADING: Cleaned_BPI_2019_Events.csv\n",
      "Size: 412.20 MB\n",
      "============================================================\n",
      "Total rows: 1,595,923\n",
      "Loading 20% sample (319,184 rows)...\n",
      "Loaded shape: (319185, 21)\n",
      "Memory usage: 43.07 MB\n",
      "\n",
      "============================================================\n",
      "LOADING: Process_Bottleneck_Analysis.csv\n",
      "Size: 0.00 MB\n",
      "============================================================\n",
      "Total rows: 42\n",
      "Loading in chunks of 100,000...\n",
      "  Loaded 100,000 rows...\n",
      "Loaded shape: (42, 2)\n",
      "Memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(filename, sample_fraction=0.3, use_dask=False):\n",
    "    \"\"\"Load dataset with memory optimization\"\"\"\n",
    "    filepath = base_path / filename\n",
    "    \n",
    "    if not filepath.exists():\n",
    "        print(f\"File not found: {filename}\")\n",
    "        # Try to find similar files\n",
    "        similar_files = list(base_path.glob(f\"*{filename.split('_')[0]}*\"))\n",
    "        if similar_files:\n",
    "            filepath = similar_files[0]\n",
    "            print(f\"Using similar file: {filepath.name}\")\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"LOADING: {filepath.name}\")\n",
    "    print(f\"Size: {filepath.stat().st_size / 1024**2:.2f} MB\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Get file extension\n",
    "    ext = filepath.suffix.lower()\n",
    "    \n",
    "    if use_dask and ext == '.csv':\n",
    "        import dask.dataframe as dd\n",
    "        print(\"Using Dask for large CSV file...\")\n",
    "        ddf = dd.read_csv(filepath)\n",
    "        print(f\"Columns: {ddf.columns.tolist()}\")\n",
    "        print(f\"Number of partitions: {ddf.npartitions}\")\n",
    "        return ddf\n",
    "    else:\n",
    "        # First, sample to understand structure\n",
    "        if ext == '.csv':\n",
    "            # Get total rows for CSV\n",
    "            with open(filepath, 'r') as f:\n",
    "                total_rows = sum(1 for line in f) - 1  # Subtract header\n",
    "            \n",
    "            print(f\"Total rows: {total_rows:,}\")\n",
    "            \n",
    "            # Read sample to determine dtypes\n",
    "            sample_size = min(10000, total_rows)\n",
    "            df_sample = pd.read_csv(filepath, nrows=sample_size)\n",
    "            \n",
    "            # Optimize dtypes\n",
    "            dtypes = {}\n",
    "            for col in df_sample.columns:\n",
    "                if df_sample[col].dtype == 'object':\n",
    "                    unique_ratio = df_sample[col].nunique() / len(df_sample)\n",
    "                    if unique_ratio < 0.5:\n",
    "                        dtypes[col] = 'category'\n",
    "                elif 'int' in str(df_sample[col].dtype):\n",
    "                    col_min = df_sample[col].min()\n",
    "                    col_max = df_sample[col].max()\n",
    "                    if col_min > -128 and col_max < 128:\n",
    "                        dtypes[col] = 'int8'\n",
    "                    elif col_min > -32768 and col_max < 32768:\n",
    "                        dtypes[col] = 'int16'\n",
    "                    elif col_min > -2147483648 and col_max < 2147483647:\n",
    "                        dtypes[col] = 'int32'\n",
    "                    else:\n",
    "                        dtypes[col] = 'int64'\n",
    "            \n",
    "            # Load data with sampling if large\n",
    "            if total_rows > 500000 and sample_fraction < 1:\n",
    "                print(f\"Loading {sample_fraction*100:.0f}% sample ({int(total_rows*sample_fraction):,} rows)...\")\n",
    "                \n",
    "                # Calculate rows to skip\n",
    "                skip_rows = np.random.choice(\n",
    "                    range(1, total_rows + 1),\n",
    "                    size=int(total_rows * (1 - sample_fraction)),\n",
    "                    replace=False\n",
    "                )\n",
    "                df = pd.read_csv(filepath, dtype=dtypes, skiprows=skip_rows)\n",
    "            else:\n",
    "                # Load in chunks\n",
    "                chunks = []\n",
    "                chunk_size = 100000\n",
    "                print(f\"Loading in chunks of {chunk_size:,}...\")\n",
    "                \n",
    "                for i, chunk in enumerate(pd.read_csv(filepath, dtype=dtypes, chunksize=chunk_size)):\n",
    "                    chunks.append(chunk)\n",
    "                    if i % 10 == 0:\n",
    "                        print(f\"  Loaded {len(chunks)*chunk_size:,} rows...\")\n",
    "                \n",
    "                df = pd.concat(chunks, ignore_index=True)\n",
    "                \n",
    "        elif ext == '.parquet':\n",
    "            df = pd.read_parquet(filepath)\n",
    "        elif ext == '.xlsx':\n",
    "            df = pd.read_excel(filepath, nrows=100000)  # Limit for Excel\n",
    "        else:\n",
    "            print(f\"Unsupported file format: {ext}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Loaded shape: {df.shape}\")\n",
    "        print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        return df\n",
    "\n",
    "# Load both datasets\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING BOTH DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load Cleaned_BPI_2019_Events\n",
    "bpi_df = load_dataset(\"Cleaned_BPI_2019_Events.csv\", sample_fraction=0.2)\n",
    "\n",
    "# Load Process_Bottleneck_Analysis\n",
    "bottleneck_df = load_dataset(\"Process_Bottleneck_Analysis.csv\", sample_fraction=1.0)  # Load full if smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37700157-1d48-40a1-b63d-fc01f72970bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE BPI 2019 ANALYSIS - STEP BY STEP\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE BPI 2019 ANALYSIS - STEP BY STEP\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42d187e0-3fcf-4d8f-bc40-c3dab43ee73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 1: LOCATE AND VERIFY DATASET\n",
      "============================================================\n",
      "File found: C:\\Users\\user\\Portfolio_Exports\\Cleaned_BPI_2019_Events.csv\n",
      "File size: 412.20 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Define file path and verify\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: LOCATE AND VERIFY DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "base_path = Path.home() / \"Portfolio_Exports\"\n",
    "file_name = \"Cleaned_BPI_2019_Events.csv\"\n",
    "file_path = base_path / file_name\n",
    "\n",
    "if not file_path.exists():\n",
    "    print(f\"File not found at: {file_path}\")\n",
    "    # Search for the file\n",
    "    files = list(base_path.glob(\"*BPI*2019*.csv\"))\n",
    "    if files:\n",
    "        file_path = files[0]\n",
    "        print(f\"Found alternative: {file_path.name}\")\n",
    "    else:\n",
    "        print(\"Searching in other locations...\")\n",
    "        search_paths = [\n",
    "            Path.home() / \"Portfolio_Export\",\n",
    "            Path.home() / \"Desktop\",\n",
    "            Path.home() / \"Documents\"\n",
    "        ]\n",
    "        for path in search_paths:\n",
    "            files = list(path.glob(\"**/*BPI*2019*.csv\"))\n",
    "            if files:\n",
    "                file_path = files[0]\n",
    "                print(f\"Found at: {file_path}\")\n",
    "                break\n",
    "else:\n",
    "    print(f\"File found: {file_path}\")\n",
    "\n",
    "print(f\"File size: {file_path.stat().st_size / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce2aa130-2699-454c-99f7-05ed127c8781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: LOAD DATA WITH OPTIMIZED MEMORY USAGE\n",
      "============================================================\n",
      "Checking file structure...\n",
      "Found 21 columns\n",
      "First few columns: ['activity', 'timestamp', 'ID', 'cCompany', 'cDocType']...\n",
      "\n",
      "Determining optimal data types...\n",
      "Determined optimal types for 21 columns\n",
      "\n",
      "Loading data in chunks...\n",
      "   Loaded 500,000 rows...\n",
      "   Loaded 1,000,000 rows...\n",
      "   Loaded 1,500,000 rows...\n",
      "\n",
      "Successfully loaded 1,595,923 rows and 21 columns\n",
      "Memory usage: 990.02 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Efficient data loading with error handling\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: LOAD DATA WITH OPTIMIZED MEMORY USAGE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def load_bpi_data_safely(file_path, sample_size=100000):\n",
    "    \"\"\"Load BPI data with proper error handling\"\"\"\n",
    "    try:\n",
    "        # First, check the file structure\n",
    "        print(\"Checking file structure...\")\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            # Read first few lines\n",
    "            header = f.readline().strip()\n",
    "            second_line = f.readline().strip()\n",
    "        \n",
    "        columns = header.split(',')\n",
    "        print(f\"Found {len(columns)} columns\")\n",
    "        print(f\"First few columns: {columns[:5]}...\")\n",
    "        \n",
    "        # Create optimized dtypes dictionary\n",
    "        print(\"\\nDetermining optimal data types...\")\n",
    "        \n",
    "        # Read a sample to understand data types\n",
    "        sample_df = pd.read_csv(file_path, nrows=1000)\n",
    "        \n",
    "        dtypes_dict = {}\n",
    "        for col in sample_df.columns:\n",
    "            col_dtype = sample_df[col].dtype\n",
    "            \n",
    "            if pd.api.types.is_object_dtype(col_dtype):\n",
    "                # Check if this looks like a date/time column\n",
    "                col_lower = col.lower()\n",
    "                if any(time_word in col_lower for time_word in ['time', 'date', 'timestamp', 'start', 'end', 'complete']):\n",
    "                    # Don't convert datetime columns to categorical\n",
    "                    dtypes_dict[col] = 'object'  # Keep as object, we'll convert later\n",
    "                else:\n",
    "                    # For other object columns, check cardinality\n",
    "                    unique_ratio = sample_df[col].nunique() / len(sample_df)\n",
    "                    if unique_ratio < 0.3:  # Less than 30% unique values\n",
    "                        dtypes_dict[col] = 'category'\n",
    "                    else:\n",
    "                        dtypes_dict[col] = 'string'\n",
    "                        \n",
    "            elif pd.api.types.is_integer_dtype(col_dtype):\n",
    "                # Downcast integers\n",
    "                col_min = sample_df[col].min()\n",
    "                col_max = sample_df[col].max()\n",
    "                \n",
    "                if col_min >= 0:  # Unsigned\n",
    "                    if col_max < 256:\n",
    "                        dtypes_dict[col] = 'uint8'\n",
    "                    elif col_max < 65536:\n",
    "                        dtypes_dict[col] = 'uint16'\n",
    "                    elif col_max < 4294967296:\n",
    "                        dtypes_dict[col] = 'uint32'\n",
    "                    else:\n",
    "                        dtypes_dict[col] = 'uint64'\n",
    "                else:  # Signed\n",
    "                    if col_min > -128 and col_max < 128:\n",
    "                        dtypes_dict[col] = 'int8'\n",
    "                    elif col_min > -32768 and col_max < 32768:\n",
    "                        dtypes_dict[col] = 'int16'\n",
    "                    elif col_min > -2147483648 and col_max < 2147483647:\n",
    "                        dtypes_dict[col] = 'int32'\n",
    "                    else:\n",
    "                        dtypes_dict[col] = 'int64'\n",
    "                        \n",
    "            elif pd.api.types.is_float_dtype(col_dtype):\n",
    "                dtypes_dict[col] = 'float32'\n",
    "            else:\n",
    "                dtypes_dict[col] = str(col_dtype)\n",
    "        \n",
    "        print(f\"Determined optimal types for {len(dtypes_dict)} columns\")\n",
    "        \n",
    "        # Load the data in chunks\n",
    "        print(\"\\nLoading data in chunks...\")\n",
    "        chunks = []\n",
    "        chunk_size = 50000\n",
    "        \n",
    "        for i, chunk in enumerate(pd.read_csv(file_path, dtype=dtypes_dict, chunksize=chunk_size)):\n",
    "            chunks.append(chunk)\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"   Loaded {(i + 1) * chunk_size:,} rows...\")\n",
    "        \n",
    "        df = pd.concat(chunks, ignore_index=True)\n",
    "        \n",
    "        print(f\"\\nSuccessfully loaded {len(df):,} rows and {len(df.columns)} columns\")\n",
    "        print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        # Try alternative loading method\n",
    "        print(\"Trying alternative loading method...\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, nrows=sample_size)\n",
    "            print(f\"Loaded sample of {len(df):,} rows\")\n",
    "            return df\n",
    "        except Exception as e2:\n",
    "            print(f\"Failed to load: {str(e2)}\")\n",
    "            return None\n",
    "\n",
    "# Load the data\n",
    "bpi_df = load_bpi_data_safely(file_path, sample_size=200000)\n",
    "\n",
    "if bpi_df is None:\n",
    "    print(\"Failed to load dataset. Exiting.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a7ee3a2-92a5-4cf2-9b24-8139917f3413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 3: INITIAL DATA INSPECTION\n",
      "============================================================\n",
      "Dataset Shape: 1,595,923 rows × 21 columns\n",
      "Column Names:\n",
      " 1. activity\n",
      " 2. timestamp\n",
      " 3. ID\n",
      " 4. cCompany\n",
      " 5. cDocType\n",
      " 6. cGR\n",
      " 7. cGRbasedInvVerif\n",
      " 8. cID\n",
      " 9. cItem\n",
      "10. cItemCat\n",
      "11. cItemType\n",
      "12. cPOID\n",
      "13. cPurDocCat\n",
      "14. cSpendAreaText\n",
      "15. cSpendClassText\n",
      "16. cSubSPendAreaText\n",
      "17. cVendor\n",
      "18. cVendorName\n",
      "19. eCumNetWorth\n",
      "20. idx\n",
      "21. resource\n",
      "\n",
      "Data Types:\n",
      "object: 9 columns\n",
      "float32: 4 columns\n",
      "bool: 2 columns\n",
      "uint64: 1 columns\n",
      "category: 1 columns\n",
      "string: 1 columns\n",
      "category: 1 columns\n",
      "category: 1 columns\n",
      "category: 1 columns\n",
      "\n",
      "First 5 rows:\n",
      "                    activity            timestamp              ID  \\\n",
      "0     Vendor creates invoice  1948-01-26 22:59:00  65781719105536   \n",
      "1  Vendor creates debit memo  1948-01-26 22:59:00  65777424138241   \n",
      "2     Vendor creates invoice  1948-01-26 22:59:00  65777424138240   \n",
      "3     Vendor creates invoice  1948-01-26 22:59:00  65794604007424   \n",
      "4  Vendor creates debit memo  1948-01-26 22:59:00  65794604007425   \n",
      "\n",
      "         cCompany     cDocType   cGR  cGRbasedInvVerif               cID  \\\n",
      "0  companyID_0000  Standard PO  True             False  4507004931_00020   \n",
      "1  companyID_0000  Standard PO  True             False  4507004931_00010   \n",
      "2  companyID_0000  Standard PO  True             False  4507004931_00010   \n",
      "3  companyID_0000  Standard PO  True             False  4507004931_00050   \n",
      "4  companyID_0000  Standard PO  True             False  4507004931_00050   \n",
      "\n",
      "   cItem                        cItemCat cItemType         cPOID  \\\n",
      "0   20.0  3-way match, invoice before GR  Standard  4.507005e+09   \n",
      "1   10.0  3-way match, invoice before GR  Standard  4.507005e+09   \n",
      "2   10.0  3-way match, invoice before GR  Standard  4.507005e+09   \n",
      "3   50.0  3-way match, invoice before GR  Standard  4.507005e+09   \n",
      "4   50.0  3-way match, invoice before GR  Standard  4.507005e+09   \n",
      "\n",
      "       cPurDocCat cSpendAreaText cSpendClassText    cSubSPendAreaText  \\\n",
      "0  Purchase order          Sales             NPR  Products for Resale   \n",
      "1  Purchase order          Sales             NPR  Products for Resale   \n",
      "2  Purchase order          Sales             NPR  Products for Resale   \n",
      "3  Purchase order          Sales             NPR  Products for Resale   \n",
      "4  Purchase order          Sales             NPR  Products for Resale   \n",
      "\n",
      "         cVendor  cVendorName  eCumNetWorth  idx resource  \n",
      "0  vendorID_0670  vendor_0645          68.0  0.0     NONE  \n",
      "1  vendorID_0670  vendor_0645         325.0  1.0     NONE  \n",
      "2  vendorID_0670  vendor_0645         325.0  2.0     NONE  \n",
      "3  vendorID_0670  vendor_0645         102.0  3.0     NONE  \n",
      "4  vendorID_0670  vendor_0645         102.0  4.0     NONE  \n",
      "\n",
      "Column Details (First 10 columns):\n",
      "\n",
      "Column: activity\n",
      " Type: object\n",
      "Unique values: 42\n",
      "Missing values: 0 (0.0%)\n",
      "\n",
      "Column: timestamp\n",
      " Type: object\n",
      "Unique values: 167,432\n",
      "Missing values: 0 (0.0%)\n",
      "\n",
      "Column: ID\n",
      " Type: uint64\n",
      "Unique values: 1,595,923\n",
      "Missing values: 0 (0.0%)\n",
      "\n",
      "Column: cCompany\n",
      " Type: object\n",
      "Unique values: 4\n",
      "Missing values: 0 (0.0%)\n",
      "Sample values: ['companyID_0000' 'companyID_0003' 'companyID_0001' 'companyID_0002']\n",
      "\n",
      "Column: cDocType\n",
      " Type: category\n",
      "Unique values: 3\n",
      "Missing values: 0 (0.0%)\n",
      "\n",
      "Column: cGR\n",
      " Type: bool\n",
      "Unique values: 2\n",
      "Missing values: 0 (0.0%)\n",
      "\n",
      "Column: cGRbasedInvVerif\n",
      " Type: bool\n",
      "Unique values: 2\n",
      "Missing values: 0 (0.0%)\n",
      "\n",
      "Column: cID\n",
      " Type: string\n",
      "Unique values: 251,734\n",
      "Missing values: 0 (0.0%)\n",
      "\n",
      "Column: cItem\n",
      " Type: float32\n",
      "Unique values: 490\n",
      "Missing values: 0 (0.0%)\n",
      "\n",
      "Column: cItemCat\n",
      " Type: category\n",
      "Unique values: 4\n",
      "Missing values: 0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Initial Data Inspection\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: INITIAL DATA INSPECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Dataset Shape: {bpi_df.shape[0]:,} rows × {bpi_df.shape[1]} columns\")\n",
    "print(f\"Column Names:\")\n",
    "for i, col in enumerate(bpi_df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "dtype_counts = bpi_df.dtypes.value_counts()\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f\"{dtype}: {count} columns\")\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(bpi_df.head())\n",
    "\n",
    "print(\"\\nColumn Details (First 10 columns):\")\n",
    "for col in bpi_df.columns[:10]:\n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(f\" Type: {bpi_df[col].dtype}\")\n",
    "    print(f\"Unique values: {bpi_df[col].nunique():,}\")\n",
    "    print(f\"Missing values: {bpi_df[col].isnull().sum():,} ({bpi_df[col].isnull().sum()/len(bpi_df)*100:.1f}%)\")\n",
    "    \n",
    "    if bpi_df[col].dtype == 'object' and bpi_df[col].nunique() < 20:\n",
    "        print(f\"Sample values: {bpi_df[col].unique()[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c950df9-b32f-4e9d-b2d3-62a95216f075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 4: IDENTIFY KEY COLUMNS FOR PROCESS MINING\n",
      "============================================================\n",
      "Identified activity: activity\n",
      "Identified timestamp: timestamp\n",
      "Identified case_id: ID\n",
      "Identified resource: resource\n",
      "\n",
      "Summary of Identified Key Columns:\n",
      "case_id     : ID (Type: uint64)\n",
      "activity    : activity (Type: object)\n",
      "timestamp   : timestamp (Type: object)\n",
      "resource    : resource (Type: object)\n",
      "cost        : NOT FOUND\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Identify Key Columns\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: IDENTIFY KEY COLUMNS FOR PROCESS MINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def identify_key_columns(df):\n",
    "    \"\"\"Identify key columns for process mining analysis\"\"\"\n",
    "    key_columns = {\n",
    "        'case_id': None,\n",
    "        'activity': None,\n",
    "        'timestamp': None,\n",
    "        'resource': None,\n",
    "        'cost': None\n",
    "    }\n",
    "    \n",
    "    # Common patterns for each type of column\n",
    "    patterns = {\n",
    "        'case_id': ['case', 'id', 'nummer', 'nr', 'nummer', 'number', 'proc', 'process'],\n",
    "        'activity': ['activity', 'event', 'action', 'task', 'step', 'handling'],\n",
    "        'timestamp': ['time', 'date', 'timestamp', 'start', 'end', 'complete', 'created', 'modified'],\n",
    "        'resource': ['resource', 'user', 'employee', 'person', 'role', 'team', 'group', 'org'],\n",
    "        'cost': ['cost', 'amount', 'price', 'revenue', 'expense', 'value', 'money']\n",
    "    }\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        \n",
    "        for key, pattern_list in patterns.items():\n",
    "            for pattern in pattern_list:\n",
    "                if pattern in col_lower:\n",
    "                    if key_columns[key] is None:\n",
    "                        key_columns[key] = col\n",
    "                        print(f\"Identified {key}: {col}\")\n",
    "                    break\n",
    "    \n",
    "    # If not found, try alternative methods\n",
    "    if key_columns['case_id'] is None:\n",
    "        # Look for columns with many unique values but not too many\n",
    "        for col in df.columns:\n",
    "            unique_ratio = df[col].nunique() / len(df)\n",
    "            if 0.001 < unique_ratio < 0.5:  # Between 0.1% and 50% unique\n",
    "                if df[col].dtype in ['object', 'string', 'category']:\n",
    "                    key_columns['case_id'] = col\n",
    "                    print(f\"Guessing {key}: {col} (unique ratio: {unique_ratio:.3f})\")\n",
    "                    break\n",
    "    \n",
    "    return key_columns\n",
    "\n",
    "key_columns = identify_key_columns(bpi_df)\n",
    "\n",
    "print(\"\\nSummary of Identified Key Columns:\")\n",
    "for key, value in key_columns.items():\n",
    "    if value:\n",
    "        print(f\"{key:12}: {value} (Type: {bpi_df[value].dtype})\")\n",
    "    else:\n",
    "        print(f\"{key:12}: NOT FOUND\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5d9190c-d956-4b4e-b96e-112b21f08404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 5: DATA TYPE CONVERSION & CLEANING\n",
      "============================================================\n",
      "Converting data types...\n",
      "   Converting 'timestamp' to datetime...\n",
      "Successfully converted 100.0% of values\n",
      "\n",
      "   Processing categorical columns...\n",
      "Converted 'activity' to ORDERED categorical (42 categories)\n",
      "Converted 'timestamp' to categorical (167432 categories)\n",
      "Converted 'cCompany' to categorical (4 categories)\n",
      "Converted 'cID' to categorical (251734 categories)\n",
      "Converted 'cSpendAreaText' to categorical (20 categories)\n",
      "Converted 'cSpendClassText' to categorical (3 categories)\n",
      "Converted 'cSubSPendAreaText' to categorical (135 categories)\n",
      "Converted 'cVendor' to categorical (1975 categories)\n",
      "Converted 'cVendorName' to categorical (1899 categories)\n",
      "Converted 'resource' to categorical (628 categories)\n",
      "\n",
      "   Optimizing numeric columns...\n",
      "\n",
      "After optimization - Memory usage: 230.75 MB\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Handle Categorical and DateTime Columns\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 5: DATA TYPE CONVERSION & CLEANING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def convert_data_types(df, key_columns):\n",
    "    \"\"\"Convert columns to proper data types with error handling\"\"\"\n",
    "    print(\"Converting data types...\")\n",
    "    \n",
    "    # 1. Convert timestamp column\n",
    "    if key_columns['timestamp']:\n",
    "        timestamp_col = key_columns['timestamp']\n",
    "        print(f\"   Converting '{timestamp_col}' to datetime...\")\n",
    "        \n",
    "        try:\n",
    "            # Handle categorical timestamps by converting to string first\n",
    "            if pd.api.types.is_categorical_dtype(df[timestamp_col]):\n",
    "                print(f\"   Note: '{timestamp_col}' is categorical. Converting to string first...\")\n",
    "                df[timestamp_col] = df[timestamp_col].astype(str)\n",
    "            \n",
    "            # Convert to datetime\n",
    "            df['_timestamp'] = pd.to_datetime(df[timestamp_col], errors='coerce')\n",
    "            \n",
    "            # Check success rate\n",
    "            success_rate = (1 - df['_timestamp'].isnull().sum() / len(df)) * 100\n",
    "            print(f\"Successfully converted {success_rate:.1f}% of values\")\n",
    "            \n",
    "            # Store original if needed\n",
    "            df['_timestamp_original'] = df[timestamp_col]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error converting timestamp: {str(e)}\")\n",
    "    \n",
    "    # 2. Handle categorical columns\n",
    "    print(\"\\n   Processing categorical columns...\")\n",
    "    for col in df.select_dtypes(include=['object', 'string']).columns:\n",
    "        if col in ['_timestamp', '_timestamp_original']:\n",
    "            continue\n",
    "            \n",
    "        unique_ratio = df[col].nunique() / len(df)\n",
    "        if unique_ratio < 0.3:  # Good candidate for categorical\n",
    "            try:\n",
    "                # Ensure the column is ordered if we might use min/max\n",
    "                if col in [key_columns['case_id'], key_columns['activity']]:\n",
    "                    df[col] = df[col].astype('category').cat.as_ordered()\n",
    "                    print(f\"Converted '{col}' to ORDERED categorical ({df[col].nunique()} categories)\")\n",
    "                else:\n",
    "                    df[col] = df[col].astype('category')\n",
    "                    print(f\"Converted '{col}' to categorical ({df[col].nunique()} categories)\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting '{col}': {str(e)}\")\n",
    "    \n",
    "    # 3. Handle numeric columns\n",
    "    print(\"\\n   Optimizing numeric columns...\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        try:\n",
    "            original_memory = df[col].memory_usage(deep=True)\n",
    "            \n",
    "            # Downcast integers\n",
    "            if pd.api.types.is_integer_dtype(df[col]):\n",
    "                col_min = df[col].min()\n",
    "                col_max = df[col].max()\n",
    "                \n",
    "                if col_min >= 0:  # Unsigned\n",
    "                    if col_max < 256:\n",
    "                        df[col] = df[col].astype('uint8')\n",
    "                    elif col_max < 65536:\n",
    "                        df[col] = df[col].astype('uint16')\n",
    "                    elif col_max < 4294967296:\n",
    "                        df[col] = df[col].astype('uint32')\n",
    "                else:  # Signed\n",
    "                    if col_min > -128 and col_max < 128:\n",
    "                        df[col] = df[col].astype('int8')\n",
    "                    elif col_min > -32768 and col_max < 32768:\n",
    "                        df[col] = df[col].astype('int16')\n",
    "                    elif col_min > -2147483648 and col_max < 2147483647:\n",
    "                        df[col] = df[col].astype('int32')\n",
    "                        \n",
    "            # Downcast floats\n",
    "            elif pd.api.types.is_float_dtype(df[col]):\n",
    "                df[col] = df[col].astype('float32')\n",
    "                \n",
    "            new_memory = df[col].memory_usage(deep=True)\n",
    "            if new_memory < original_memory:\n",
    "                print(f\"Optimized '{col}' memory usage\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error optimizing '{col}': {str(e)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Convert data types\n",
    "bpi_df = convert_data_types(bpi_df, key_columns)\n",
    "\n",
    "print(f\"\\nAfter optimization - Memory usage: {bpi_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "955d242b-91d9-4809-aca8-fdb27f126af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 6: BASIC PROCESS MINING ANALYSIS\n",
      "============================================================\n",
      "1. CASE ANALYSIS (using 'ID')\n",
      "Total unique cases: 1,595,923\n",
      "\n",
      "Case Length Statistics:\n",
      "Minimum events per case: 1\n",
      "Maximum events per case: 1\n",
      "Average events per case: 1.00\n",
      "Median events per case: 1.00\n",
      "Standard deviation: 0.00\n",
      "\n",
      "Case Length Distribution:\n",
      "10th percentile: 1.0 events\n",
      "25th percentile: 1.0 events\n",
      "50th percentile: 1.0 events\n",
      "75th percentile: 1.0 events\n",
      "90th percentile: 1.0 events\n",
      "95th percentile: 1.0 events\n",
      "99th percentile: 1.0 events\n",
      "\n",
      "2. ACTIVITY ANALYSIS (using 'activity')\n",
      "   Total unique activities: 42\n",
      "\n",
      "Top 15 Most Frequent Activities:\n",
      " 1. Record Goods Receipt                                 314,097 (19.7%)\n",
      " 2. Create Purchase Order Item                           251,734 (15.8%)\n",
      " 3. Record Invoice Receipt                               228,760 (14.3%)\n",
      " 4. Vendor creates invoice                               219,919 (13.8%)\n",
      " 5. Clear Invoice                                        194,393 (12.2%)\n",
      " 6. Record Service Entry Sheet                           164,975 (10.3%)\n",
      " 7. Remove Payment Block                                  57,136 (3.6%)\n",
      " 8. Create Purchase Requisition Item                      46,592 (2.9%)\n",
      " 9. Receive Order Confirmation                            32,065 (2.0%)\n",
      "10. Change Quantity                                       21,449 (1.3%)\n",
      "11. Change Price                                          12,423 (0.8%)\n",
      "12. Delete Purchase Order Item                             8,875 (0.6%)\n",
      "13. Change Approval for Purchase Order                     7,541 (0.5%)\n",
      "14. Cancel Invoice Receipt                                 7,096 (0.4%)\n",
      "15. Vendor creates debit memo                              6,255 (0.4%)\n",
      "\n",
      "Pareto Analysis:\n",
      "Top 6 activities account for 80% of all events\n",
      "That's 14.3% of activities causing 80% of work\n",
      "\n",
      "3. TEMPORAL ANALYSIS\n",
      "Time period covered: 26372 days\n",
      "From: 1948-01-26 22:59:00\n",
      "To:   2020-04-09 22:59:00\n",
      "\n",
      "Daily Event Statistics:\n",
      "Average events per day: 3311\n",
      "Maximum events in a day: 16,892\n",
      "Minimum events in a day: 1\n",
      "Busiest day: 2018-08-30 with 16,892 events\n",
      "\n",
      "Events by Day of Week:\n",
      "Monday    :   253,948 (15.9%)\n",
      "Tuesday   :   283,162 (17.7%)\n",
      "Wednesday :   293,913 (18.4%)\n",
      "Thursday  :   426,041 (26.7%)\n",
      "Friday    :   291,329 (18.3%)\n",
      "Saturday  :    35,887 (2.2%)\n",
      "Sunday    :    11,643 (0.7%)\n",
      "\n",
      "Events by Hour of Day:\n",
      "     00:00 - 00:59:     1,868 (0.1%)\n",
      "     01:00 - 01:59:    66,573 (4.2%)\n",
      "     02:00 - 02:59:     1,126 (0.1%)\n",
      "     03:00 - 03:59:     7,565 (0.5%)\n",
      "     04:00 - 04:59:     5,573 (0.3%)\n",
      "     05:00 - 05:59:     8,315 (0.5%)\n",
      "     06:00 - 06:59:    45,506 (2.9%)\n",
      "     07:00 - 07:59:    70,011 (4.4%)\n",
      "     08:00 - 08:59:    95,967 (6.0%)\n",
      "     09:00 - 09:59:   107,795 (6.8%)\n",
      "     10:00 - 10:59:   117,849 (7.4%)\n",
      "     11:00 - 11:59:    81,026 (5.1%)\n",
      "     12:00 - 12:59:   106,543 (6.7%)\n",
      "     13:00 - 13:59:   121,487 (7.6%)\n",
      "     14:00 - 14:59:   105,416 (6.6%)\n",
      "     15:00 - 15:59:    99,817 (6.3%)\n",
      "     16:00 - 16:59:    78,378 (4.9%)\n",
      "     17:00 - 17:59:    30,903 (1.9%)\n",
      "     18:00 - 18:59:     5,958 (0.4%)\n",
      "     19:00 - 19:59:     3,962 (0.2%)\n",
      "     20:00 - 20:59:     2,831 (0.2%)\n",
      "     21:00 - 21:59:    58,698 (3.7%)\n",
      "     22:00 - 22:59:   310,072 (19.4%)\n",
      "     23:00 - 23:59:    62,684 (3.9%)\n",
      "\n",
      "4. RESOURCE ANALYSIS (using 'resource')\n",
      "Total unique resources: 628\n",
      "\n",
      "Top 15 Busiest Resources:\n",
      " 1. NONE                                     399,090 (25.0%)\n",
      " 2. user_002                                 166,353 (10.4%)\n",
      " 3. user_029                                 71,539 (4.5%)\n",
      " 4. user_020                                 39,770 (2.5%)\n",
      " 5. batch_06                                 38,100 (2.4%)\n",
      " 6. user_013                                 35,069 (2.2%)\n",
      " 7. user_001                                 34,563 (2.2%)\n",
      " 8. user_012                                 32,707 (2.0%)\n",
      " 9. user_019                                 29,764 (1.9%)\n",
      "10. user_235                                 28,336 (1.8%)\n",
      "11. user_015                                 27,941 (1.8%)\n",
      "12. batch_00                                 27,934 (1.8%)\n",
      "13. batch_02                                 27,933 (1.8%)\n",
      "14. user_006                                 17,636 (1.1%)\n",
      "15. user_040                                 17,627 (1.1%)\n",
      "\n",
      "Resource Utilization Imbalance:\n",
      "Busiest resource: NONE with 399,090 events\n",
      "Quietest resource: user_606 with 1 events\n",
      "Imbalance ratio: 399090.0x\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Basic Process Mining Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 6: BASIC PROCESS MINING ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def basic_process_analysis(df, key_columns):\n",
    "    \"\"\"Perform basic process mining analysis\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Case Analysis\n",
    "    if key_columns['case_id']:\n",
    "        case_col = key_columns['case_id']\n",
    "        print(f\"1. CASE ANALYSIS (using '{case_col}')\")\n",
    "        \n",
    "        n_cases = df[case_col].nunique()\n",
    "        print(f\"Total unique cases: {n_cases:,}\")\n",
    "        \n",
    "        # Case length distribution\n",
    "        case_lengths = df.groupby(case_col).size()\n",
    "        \n",
    "        print(f\"\\nCase Length Statistics:\")\n",
    "        print(f\"Minimum events per case: {case_lengths.min()}\")\n",
    "        print(f\"Maximum events per case: {case_lengths.max()}\")\n",
    "        print(f\"Average events per case: {case_lengths.mean():.2f}\")\n",
    "        print(f\"Median events per case: {case_lengths.median():.2f}\")\n",
    "        print(f\"Standard deviation: {case_lengths.std():.2f}\")\n",
    "        \n",
    "        # Distribution percentiles\n",
    "        print(f\"\\nCase Length Distribution:\")\n",
    "        for p in [10, 25, 50, 75, 90, 95, 99]:\n",
    "            percentile = case_lengths.quantile(p/100)\n",
    "            print(f\"{p}th percentile: {percentile:.1f} events\")\n",
    "        \n",
    "        results['case_stats'] = {\n",
    "            'total_cases': n_cases,\n",
    "            'case_lengths': case_lengths\n",
    "        }\n",
    "    \n",
    "    # 2. Activity Analysis\n",
    "    if key_columns['activity']:\n",
    "        activity_col = key_columns['activity']\n",
    "        print(f\"\\n2. ACTIVITY ANALYSIS (using '{activity_col}')\")\n",
    "        \n",
    "        n_activities = df[activity_col].nunique()\n",
    "        print(f\"   Total unique activities: {n_activities}\")\n",
    "        \n",
    "        # Frequency of activities\n",
    "        activity_counts = df[activity_col].value_counts()\n",
    "        \n",
    "        print(f\"\\nTop 15 Most Frequent Activities:\")\n",
    "        top_n = 15\n",
    "        for i, (activity, count) in enumerate(activity_counts.head(top_n).items(), 1):\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"{i:2d}. {str(activity)[:50]:50} {count:9,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Pareto analysis (80/20 rule)\n",
    "        total_events = len(df)\n",
    "        cumulative_sum = 0\n",
    "        pareto_activities = 0\n",
    "        \n",
    "        for i, (activity, count) in enumerate(activity_counts.items(), 1):\n",
    "            cumulative_sum += count\n",
    "            if cumulative_sum / total_events >= 0.8:\n",
    "                pareto_activities = i\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nPareto Analysis:\")\n",
    "        print(f\"Top {pareto_activities} activities account for 80% of all events\")\n",
    "        print(f\"That's {pareto_activities/n_activities*100:.1f}% of activities causing 80% of work\")\n",
    "        \n",
    "        results['activity_stats'] = {\n",
    "            'total_activities': n_activities,\n",
    "            'activity_counts': activity_counts\n",
    "        }\n",
    "    \n",
    "    # 3. Temporal Analysis\n",
    "    if '_timestamp' in df.columns and df['_timestamp'].notnull().any():\n",
    "        print(f\"\\n3. TEMPORAL ANALYSIS\")\n",
    "        \n",
    "        # Ensure datetime is properly ordered\n",
    "        df = df.sort_values('_timestamp')\n",
    "        \n",
    "        time_range = df['_timestamp'].max() - df['_timestamp'].min()\n",
    "        print(f\"Time period covered: {time_range.days} days\")\n",
    "        print(f\"From: {df['_timestamp'].min()}\")\n",
    "        print(f\"To:   {df['_timestamp'].max()}\")\n",
    "        \n",
    "        # Events over time\n",
    "        df['_date'] = df['_timestamp'].dt.date\n",
    "        daily_counts = df.groupby('_date').size()\n",
    "        \n",
    "        print(f\"\\nDaily Event Statistics:\")\n",
    "        print(f\"Average events per day: {daily_counts.mean():.0f}\")\n",
    "        print(f\"Maximum events in a day: {daily_counts.max():,}\")\n",
    "        print(f\"Minimum events in a day: {daily_counts.min():,}\")\n",
    "        print(f\"Busiest day: {daily_counts.idxmax()} with {daily_counts.max():,} events\")\n",
    "        \n",
    "        # Day of week analysis\n",
    "        df['_dayofweek'] = df['_timestamp'].dt.dayofweek\n",
    "        df['_dayname'] = df['_timestamp'].dt.day_name()\n",
    "        \n",
    "        print(f\"\\nEvents by Day of Week:\")\n",
    "        day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        for day in day_order:\n",
    "            count = (df['_dayname'] == day).sum()\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"{day:10}: {count:9,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Hourly analysis\n",
    "        df['_hour'] = df['_timestamp'].dt.hour\n",
    "        print(f\"\\nEvents by Hour of Day:\")\n",
    "        for hour in range(24):\n",
    "            count = (df['_hour'] == hour).sum()\n",
    "            if count > 0:\n",
    "                percentage = (count / len(df)) * 100\n",
    "                print(f\"     {hour:02d}:00 - {hour:02d}:59: {count:9,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        results['temporal_stats'] = {\n",
    "            'time_range': time_range,\n",
    "            'daily_counts': daily_counts\n",
    "        }\n",
    "    \n",
    "    # 4. Resource Analysis (if available)\n",
    "    if key_columns['resource']:\n",
    "        resource_col = key_columns['resource']\n",
    "        print(f\"\\n4. RESOURCE ANALYSIS (using '{resource_col}')\")\n",
    "        \n",
    "        n_resources = df[resource_col].nunique()\n",
    "        print(f\"Total unique resources: {n_resources}\")\n",
    "        \n",
    "        resource_counts = df[resource_col].value_counts()\n",
    "        \n",
    "        print(f\"\\nTop 15 Busiest Resources:\")\n",
    "        for i, (resource, count) in enumerate(resource_counts.head(15).items(), 1):\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"{i:2d}. {str(resource)[:40]:40} {count:6,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Resource utilization imbalance\n",
    "        if len(resource_counts) > 1:\n",
    "            busiest = resource_counts.iloc[0]\n",
    "            quietest = resource_counts.iloc[-1]\n",
    "            imbalance_ratio = busiest / quietest if quietest > 0 else float('inf')\n",
    "            \n",
    "            print(f\"\\nResource Utilization Imbalance:\")\n",
    "            print(f\"Busiest resource: {resource_counts.index[0]} with {busiest:,} events\")\n",
    "            print(f\"Quietest resource: {resource_counts.index[-1]} with {quietest:,} events\")\n",
    "            print(f\"Imbalance ratio: {imbalance_ratio:.1f}x\")\n",
    "        \n",
    "        results['resource_stats'] = {\n",
    "            'total_resources': n_resources,\n",
    "            'resource_counts': resource_counts\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Perform basic analysis\n",
    "analysis_results = basic_process_analysis(bpi_df, key_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "799d83b3-dabb-4912-b3b6-a6fd6b7736d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 7: ADVANCED ANALYSIS - PROCESS VARIANTS\n",
      "============================================================\n",
      "Identifying process variants...\n",
      "Creating case traces...\n",
      "Analyzing trace variants...\n",
      "\n",
      "Process Variant Analysis:\n",
      "Total unique process variants: 42\n",
      "Average cases per variant: 37998.2\n",
      "\n",
      "Top 10 Most Common Variants:\n",
      "\n",
      " 1. Variant #1\n",
      "Cases: 314,097 (19.7%)\n",
      "Path: Record Goods Receipt\n",
      "\n",
      " 2. Variant #2\n",
      "Cases: 251,734 (15.8%)\n",
      "Path: Create Purchase Order Item\n",
      "\n",
      " 3. Variant #3\n",
      "Cases: 228,760 (14.3%)\n",
      "Path: Record Invoice Receipt\n",
      "\n",
      " 4. Variant #4\n",
      "Cases: 219,919 (13.8%)\n",
      "Path: Vendor creates invoice\n",
      "\n",
      " 5. Variant #5\n",
      "Cases: 194,393 (12.2%)\n",
      "Path: Clear Invoice\n",
      "\n",
      " 6. Variant #6\n",
      "Cases: 164,975 (10.3%)\n",
      "Path: Record Service Entry Sheet\n",
      "\n",
      " 7. Variant #7\n",
      "Cases: 57,136 (3.6%)\n",
      "Path: Remove Payment Block\n",
      "\n",
      " 8. Variant #8\n",
      "Cases: 46,592 (2.9%)\n",
      "Path: Create Purchase Requisition Item\n",
      "\n",
      " 9. Variant #9\n",
      "Cases: 32,065 (2.0%)\n",
      "Path: Receive Order Confirmation\n",
      "\n",
      "10. Variant #10\n",
      "Cases: 21,449 (1.3%)\n",
      "Path: Change Quantity\n",
      "\n",
      "Variant Length Statistics:\n",
      "Shortest variant: 1 activities\n",
      "Longest variant: 1 activities\n",
      "Average variant length: 1.0 activities\n",
      "\n",
      "Process Complexity Metrics:\n",
      "Variants covering 50% of cases: 3\n",
      "Variants covering 80% of cases: 5\n",
      "Variants covering 95% of cases: 9\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Advanced Process Mining - Process Variants\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 7: ADVANCED ANALYSIS - PROCESS VARIANTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def analyze_process_variants(df, key_columns):\n",
    "    \"\"\"Analyze different process paths/variants\"\"\"\n",
    "    \n",
    "    if not (key_columns['case_id'] and key_columns['activity']):\n",
    "        print(\"Missing case or activity column for variant analysis\")\n",
    "        return None\n",
    "    \n",
    "    case_col = key_columns['case_id']\n",
    "    activity_col = key_columns['activity']\n",
    "    \n",
    "    print(f\"Identifying process variants...\")\n",
    "    \n",
    "    try:\n",
    "        # Sort by case and timestamp\n",
    "        if '_timestamp' in df.columns:\n",
    "            df_sorted = df.sort_values([case_col, '_timestamp'])\n",
    "        else:\n",
    "            df_sorted = df.sort_values(case_col)\n",
    "        \n",
    "        # Create trace for each case\n",
    "        print(\"Creating case traces...\")\n",
    "        traces = df_sorted.groupby(case_col)[activity_col].apply(list)\n",
    "        \n",
    "        # Convert trace to string representation\n",
    "        print(\"Analyzing trace variants...\")\n",
    "        trace_strings = traces.apply(lambda x: ' -> '.join(map(str, x)))\n",
    "        \n",
    "        # Count variants\n",
    "        variant_counts = trace_strings.value_counts()\n",
    "        n_variants = len(variant_counts)\n",
    "        \n",
    "        print(f\"\\nProcess Variant Analysis:\")\n",
    "        print(f\"Total unique process variants: {n_variants:,}\")\n",
    "        print(f\"Average cases per variant: {len(traces) / n_variants:.1f}\")\n",
    "        \n",
    "        # Most common variants\n",
    "        print(f\"\\nTop 10 Most Common Variants:\")\n",
    "        total_cases = len(traces)\n",
    "        for i, (variant, count) in enumerate(variant_counts.head(10).items(), 1):\n",
    "            percentage = (count / total_cases) * 100\n",
    "            \n",
    "            # Truncate long variants for display\n",
    "            if len(variant) > 100:\n",
    "                display_variant = variant[:97] + \"...\"\n",
    "            else:\n",
    "                display_variant = variant\n",
    "            \n",
    "            print(f\"\\n{i:2d}. Variant #{i}\")\n",
    "            print(f\"Cases: {count:,} ({percentage:.1f}%)\")\n",
    "            print(f\"Path: {display_variant}\")\n",
    "        \n",
    "        # Variant length analysis\n",
    "        trace_lengths = traces.apply(len)\n",
    "        variant_length_stats = trace_lengths.groupby(trace_strings).first()\n",
    "        \n",
    "        print(f\"\\nVariant Length Statistics:\")\n",
    "        print(f\"Shortest variant: {variant_length_stats.min()} activities\")\n",
    "        print(f\"Longest variant: {variant_length_stats.max()} activities\")\n",
    "        print(f\"Average variant length: {variant_length_stats.mean():.1f} activities\")\n",
    "        \n",
    "        # Complexity analysis\n",
    "        print(f\"\\nProcess Complexity Metrics:\")\n",
    "        print(f\"Variants covering 50% of cases: {len(variant_counts[variant_counts.cumsum() <= total_cases * 0.5]):,}\")\n",
    "        print(f\"Variants covering 80% of cases: {len(variant_counts[variant_counts.cumsum() <= total_cases * 0.8]):,}\")\n",
    "        print(f\"Variants covering 95% of cases: {len(variant_counts[variant_counts.cumsum() <= total_cases * 0.95]):,}\")\n",
    "        \n",
    "        return {\n",
    "            'traces': traces,\n",
    "            'variant_counts': variant_counts,\n",
    "            'variant_length_stats': variant_length_stats\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in variant analysis: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Analyze process variants\n",
    "variant_results = analyze_process_variants(bpi_df, key_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efcac238-4fbc-4b8e-abdb-c0a79b87b58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 8: PERFORMANCE & DURATION ANALYSIS\n",
      "============================================================\n",
      "Calculating case performance metrics...\n",
      "\n",
      "Case Duration Analysis:\n",
      "Cases analyzed: 1,595,923\n",
      "\n",
      "Duration Statistics (hours):\n",
      "Minimum:    0.0\n",
      "Maximum:    0.0\n",
      "Mean:       0.0\n",
      "Median:     0.0\n",
      "Std Dev:    0.0\n",
      "\n",
      "Duration Percentiles (hours):\n",
      "10th percentile: 0.0\n",
      "25th percentile: 0.0\n",
      "50th percentile: 0.0\n",
      "75th percentile: 0.0\n",
      "90th percentile: 0.0\n",
      "95th percentile: 0.0\n",
      "99th percentile: 0.0\n",
      "\n",
      "Long-running Cases (top 5%):\n",
      "Threshold: > 0.0 hours\n",
      "Number of cases: 0 (0.0%)\n",
      "\n",
      "Process Throughput Analysis:\n",
      "Total days observed: 26372\n",
      "Average cases per day: 60.5\n",
      "Average cycle time: 0.0 days\n",
      "\n",
      "Identifying Bottleneck Activities...\n",
      "\n",
      "Top 10 Activities with Longest Average Wait Times:\n",
      " 1. Block Purchase Order Item                             nanh (median: nanh, n=0.0)\n",
      " 2. Cancel Goods Receipt                                  nanh (median: nanh, n=0.0)\n",
      " 3. Cancel Invoice Receipt                                nanh (median: nanh, n=0.0)\n",
      " 4. Cancel Subsequent Invoice                             nanh (median: nanh, n=0.0)\n",
      " 5. Change Approval for Purchase Order                    nanh (median: nanh, n=0.0)\n",
      " 6. Change Currency                                       nanh (median: nanh, n=0.0)\n",
      " 7. Change Delivery Indicator                             nanh (median: nanh, n=0.0)\n",
      " 8. Change Final Invoice Indicator                        nanh (median: nanh, n=0.0)\n",
      " 9. Change Price                                          nanh (median: nanh, n=0.0)\n",
      "10. Change Quantity                                       nanh (median: nanh, n=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Performance Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 8: PERFORMANCE & DURATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def analyze_performance(df, key_columns):\n",
    "    \"\"\"Analyze performance metrics and bottlenecks\"\"\"\n",
    "    \n",
    "    if not (key_columns['case_id'] and '_timestamp' in df.columns):\n",
    "        print(\"Missing case or timestamp data for performance analysis\")\n",
    "        return None\n",
    "    \n",
    "    case_col = key_columns['case_id']\n",
    "    \n",
    "    print(f\"Calculating case performance metrics...\")\n",
    "    \n",
    "    try:\n",
    "        # Ensure proper ordering for timestamps\n",
    "        df_sorted = df.sort_values([case_col, '_timestamp'])\n",
    "        \n",
    "        # Calculate case start and end times\n",
    "        case_times = df_sorted.groupby(case_col)['_timestamp'].agg(['min', 'max', 'count'])\n",
    "        case_times.columns = ['start_time', 'end_time', 'event_count']\n",
    "        \n",
    "        # Calculate duration\n",
    "        case_times['duration_hours'] = (case_times['end_time'] - case_times['start_time']).dt.total_seconds() / 3600\n",
    "        \n",
    "        print(f\"\\nCase Duration Analysis:\")\n",
    "        print(f\"Cases analyzed: {len(case_times):,}\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        duration_stats = case_times['duration_hours'].describe()\n",
    "        print(f\"\\nDuration Statistics (hours):\")\n",
    "        print(f\"Minimum:    {duration_stats['min']:.1f}\")\n",
    "        print(f\"Maximum:    {duration_stats['max']:.1f}\")\n",
    "        print(f\"Mean:       {duration_stats['mean']:.1f}\")\n",
    "        print(f\"Median:     {case_times['duration_hours'].median():.1f}\")\n",
    "        print(f\"Std Dev:    {duration_stats['std']:.1f}\")\n",
    "        \n",
    "        # Percentile analysis\n",
    "        print(f\"\\nDuration Percentiles (hours):\")\n",
    "        for p in [10, 25, 50, 75, 90, 95, 99]:\n",
    "            percentile = case_times['duration_hours'].quantile(p/100)\n",
    "            print(f\"{p}th percentile: {percentile:.1f}\")\n",
    "        \n",
    "        # Identify long-running cases\n",
    "        threshold_95 = case_times['duration_hours'].quantile(0.95)\n",
    "        long_cases = case_times[case_times['duration_hours'] > threshold_95]\n",
    "        \n",
    "        print(f\"\\nLong-running Cases (top 5%):\")\n",
    "        print(f\"Threshold: > {threshold_95:.1f} hours\")\n",
    "        print(f\"Number of cases: {len(long_cases):,} ({len(long_cases)/len(case_times)*100:.1f}%)\")\n",
    "        \n",
    "        if len(long_cases) > 0:\n",
    "            print(f\"Example long case IDs: {long_cases.index[:5].tolist()}\")\n",
    "        \n",
    "        # Throughput analysis\n",
    "        print(f\"\\nProcess Throughput Analysis:\")\n",
    "        \n",
    "        if len(case_times) > 1 and (case_times['end_time'].max() - case_times['start_time'].min()).days > 0:\n",
    "            total_days = (case_times['end_time'].max() - case_times['start_time'].min()).days\n",
    "            avg_cases_per_day = len(case_times) / total_days\n",
    "            \n",
    "            print(f\"Total days observed: {total_days}\")\n",
    "            print(f\"Average cases per day: {avg_cases_per_day:.1f}\")\n",
    "            print(f\"Average cycle time: {case_times['duration_hours'].mean() / 24:.1f} days\")\n",
    "        \n",
    "        # Bottleneck identification (waiting times between activities)\n",
    "        if key_columns['activity']:\n",
    "            activity_col = key_columns['activity']\n",
    "            print(f\"\\nIdentifying Bottleneck Activities...\")\n",
    "            \n",
    "            # Calculate waiting time between consecutive events in same case\n",
    "            df_sorted['next_time'] = df_sorted.groupby(case_col)['_timestamp'].shift(-1)\n",
    "            df_sorted['wait_time_hours'] = (df_sorted['next_time'] - df_sorted['_timestamp']).dt.total_seconds() / 3600\n",
    "            \n",
    "            # Filter out negative wait times (shouldn't happen if sorted correctly)\n",
    "            valid_waits = df_sorted[df_sorted['wait_time_hours'] >= 0]\n",
    "            \n",
    "            # Average wait time per activity\n",
    "            activity_wait_times = valid_waits.groupby(activity_col)['wait_time_hours'].agg(['mean', 'median', 'std', 'count'])\n",
    "            activity_wait_times = activity_wait_times.sort_values('mean', ascending=False)\n",
    "            \n",
    "            print(f\"\\nTop 10 Activities with Longest Average Wait Times:\")\n",
    "            for i, (activity, row) in enumerate(activity_wait_times.head(10).iterrows(), 1):\n",
    "                print(f\"{i:2d}. {str(activity)[:50]:50} {row['mean']:6.1f}h (median: {row['median']:.1f}h, n={row['count']:,})\")\n",
    "        \n",
    "        return {\n",
    "            'case_times': case_times,\n",
    "            'duration_stats': duration_stats,\n",
    "            'activity_wait_times': activity_wait_times if 'activity_wait_times' in locals() else None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in performance analysis: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Analyze performance\n",
    "performance_results = analyze_performance(bpi_df, key_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e82f41bb-42a4-45eb-9272-486010e4ed43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 9: DATA QUALITY ASSESSMENT\n",
      "============================================================\n",
      "Assessing data quality...\n",
      "\n",
      "1. Missing Values Analysis:\n",
      "Total missing values: 48,882\n",
      "Percentage of all data: 0.13%\n",
      "\n",
      "   Columns with missing values:\n",
      "     cSpendAreaText                    16,294 (1.0%)\n",
      "     cSpendClassText                   16,294 (1.0%)\n",
      "     cSubSPendAreaText                 16,294 (1.0%)\n",
      "\n",
      "2. Duplicate Analysis:\n",
      "Exact duplicate rows: 0 (0.00%)\n",
      "Duplicate events (same case, activity, timestamp): 0\n",
      "\n",
      "3. Timestamp Consistency:\n",
      "Timestamp order violations: 0\n",
      "   Future dates: 0\n",
      "   Very old dates found (earliest: 1948-01-26 22:59:00)\n",
      "\n",
      "4. Case Completeness:\n",
      "Cases with only 1 event: 1,595,923\n",
      "\n",
      "========================================\n",
      "DATA QUALITY SUMMARY\n",
      "========================================\n",
      "Issues found:\n",
      "   • Many cases (1,595,923) have only 1 event\n",
      "\n",
      "Overall Data Quality Score: 99.7/100\n",
      "Excellent data quality!\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Data Quality Assessment\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 9: DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def assess_data_quality(df, key_columns):\n",
    "    \"\"\"Assess data quality and identify issues\"\"\"\n",
    "    \n",
    "    print(\"Assessing data quality...\")\n",
    "    \n",
    "    quality_issues = []\n",
    "    \n",
    "    # 1. Missing values\n",
    "    print(f\"\\n1. Missing Values Analysis:\")\n",
    "    missing_total = df.isnull().sum().sum()\n",
    "    missing_percentage = (missing_total / (df.shape[0] * df.shape[1])) * 100\n",
    "    \n",
    "    print(f\"Total missing values: {missing_total:,}\")\n",
    "    print(f\"Percentage of all data: {missing_percentage:.2f}%\")\n",
    "    \n",
    "    if missing_total > 0:\n",
    "        print(f\"\\n   Columns with missing values:\")\n",
    "        missing_by_col = df.isnull().sum()\n",
    "        missing_by_col = missing_by_col[missing_by_col > 0].sort_values(ascending=False)\n",
    "        \n",
    "        for col, count in missing_by_col.head(10).items():\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"     {col:30} {count:9,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # 2. Duplicates\n",
    "    print(f\"\\n2. Duplicate Analysis:\")\n",
    "    \n",
    "    # Exact duplicate rows\n",
    "    exact_duplicates = df.duplicated().sum()\n",
    "    print(f\"Exact duplicate rows: {exact_duplicates:,} ({exact_duplicates/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Duplicate events within cases\n",
    "    if key_columns['case_id'] and key_columns['activity'] and '_timestamp' in df.columns:\n",
    "        case_col = key_columns['case_id']\n",
    "        activity_col = key_columns['activity']\n",
    "        \n",
    "        duplicate_events = df.duplicated(subset=[case_col, activity_col, '_timestamp']).sum()\n",
    "        print(f\"Duplicate events (same case, activity, timestamp): {duplicate_events:,}\")\n",
    "        \n",
    "        if duplicate_events > 0:\n",
    "            quality_issues.append(f\"Found {duplicate_events:,} duplicate events\")\n",
    "    \n",
    "    # 3. Timestamp consistency\n",
    "    print(f\"\\n3. Timestamp Consistency:\")\n",
    "    \n",
    "    if '_timestamp' in df.columns:\n",
    "        # Check for chronological order within cases\n",
    "        if key_columns['case_id']:\n",
    "            df_sorted = df.sort_values([key_columns['case_id'], '_timestamp'])\n",
    "            df_sorted['next_time'] = df_sorted.groupby(key_columns['case_id'])['_timestamp'].shift(-1)\n",
    "            \n",
    "            time_violations = (df_sorted['next_time'] < df_sorted['_timestamp']).sum()\n",
    "            print(f\"Timestamp order violations: {time_violations:,}\")\n",
    "            \n",
    "            if time_violations > 0:\n",
    "                quality_issues.append(f\"Found {time_violations:,} timestamp order violations\")\n",
    "        \n",
    "        # Check for future dates\n",
    "        now = pd.Timestamp.now()\n",
    "        future_dates = df[df['_timestamp'] > now].shape[0]\n",
    "        print(f\"   Future dates: {future_dates:,}\")\n",
    "        \n",
    "        # Check for unrealistic dates (too far in past)\n",
    "        min_date = df['_timestamp'].min()\n",
    "        if min_date.year < 2000:\n",
    "            print(f\"   Very old dates found (earliest: {min_date})\")\n",
    "    \n",
    "    # 4. Case completeness\n",
    "    print(f\"\\n4. Case Completeness:\")\n",
    "    \n",
    "    if key_columns['case_id']:\n",
    "        case_col = key_columns['case_id']\n",
    "        cases_with_single_event = (df.groupby(case_col).size() == 1).sum()\n",
    "        print(f\"Cases with only 1 event: {cases_with_single_event:,}\")\n",
    "        \n",
    "        if cases_with_single_event > len(df[case_col].unique()) * 0.1:\n",
    "            quality_issues.append(f\"Many cases ({cases_with_single_event:,}) have only 1 event\")\n",
    "    \n",
    "    # 5. Summary\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(\"DATA QUALITY SUMMARY\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    if quality_issues:\n",
    "        print(\"Issues found:\")\n",
    "        for issue in quality_issues:\n",
    "            print(f\"   • {issue}\")\n",
    "    else:\n",
    "        print(\"No major data quality issues found\")\n",
    "    \n",
    "    # Calculate quality score\n",
    "    quality_score = 100\n",
    "    \n",
    "    # Penalize for missing values\n",
    "    quality_score -= min(20, missing_percentage * 2)\n",
    "    \n",
    "    # Penalize for duplicates\n",
    "    if exact_duplicates > 0:\n",
    "        duplicate_percentage = (exact_duplicates / len(df)) * 100\n",
    "        quality_score -= min(15, duplicate_percentage * 3)\n",
    "    \n",
    "    # Penalize for time violations\n",
    "    if time_violations > 0:\n",
    "        violation_percentage = (time_violations / len(df)) * 100\n",
    "        quality_score -= min(10, violation_percentage * 5)\n",
    "    \n",
    "    quality_score = max(0, quality_score)\n",
    "    \n",
    "    print(f\"\\nOverall Data Quality Score: {quality_score:.1f}/100\")\n",
    "    \n",
    "    if quality_score >= 90:\n",
    "        print(\"Excellent data quality!\")\n",
    "    elif quality_score >= 80:\n",
    "        print(\"Good data quality\")\n",
    "    elif quality_score >= 70:\n",
    "        print(\"Acceptable data quality, some issues noted\")\n",
    "    else:\n",
    "        print(\"Data quality needs improvement\")\n",
    "\n",
    "# Assess data quality\n",
    "assess_data_quality(bpi_df, key_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f3860cd-231a-4d59-9dd3-86d3faec7896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 10: SUMMARY & RECOMMENDATIONS\n",
      "============================================================\n",
      "ANALYSIS SUMMARY\n",
      "----------------------------------------\n",
      "Total Cases: 1,595,923\n",
      "Total Activities: 42\n",
      "Time Period: 26372 days\n",
      "Avg Case Duration: 0.0 hours\n",
      "Median Duration: 0.0 hours\n",
      "\n",
      "ACTIONABLE RECOMMENDATIONS\n",
      "----------------------------------------\n",
      "1. Significant resource utilization imbalance detected. Consider workload balancing.\n",
      "\n",
      "SUGGESTED NEXT STEPS\n",
      "----------------------------------------\n",
      "1. Implement real-time monitoring for cases exceeding duration thresholds\n",
      "2. Create automated alerts for bottleneck activities\n",
      "3. Develop dashboard showing: Case throughput, Resource utilization, Bottleneck status\n",
      "4. Schedule regular process reviews based on these metrics\n",
      "5. Consider process automation for high-frequency, repetitive activities\n",
      "\n",
      "EXPORTING RESULTS\n",
      "----------------------------------------\n",
      "Exported activity frequencies\n",
      "Exported case durations\n",
      "Exported process variants\n",
      "\n",
      "All results exported to: C:\\Users\\user\\Portfolio_Exports\\BPI_Analysis_Results\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS COMPLETE!\n",
      "================================================================================\n",
      "Successfully completed comprehensive BPI 2019 analysis\n",
      "Analyzed data with proper handling of categorical ordering\n",
      "Generated actionable insights and recommendations\n",
      "Results available for further analysis in Tableau\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Summary and Recommendations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 10: SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def generate_summary_and_recommendations(analysis_results, variant_results, performance_results):\n",
    "    \"\"\"Generate summary and actionable recommendations\"\"\"\n",
    "    \n",
    "    print(\"ANALYSIS SUMMARY\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    if 'case_stats' in analysis_results:\n",
    "        print(f\"Total Cases: {analysis_results['case_stats']['total_cases']:,}\")\n",
    "    \n",
    "    if 'activity_stats' in analysis_results:\n",
    "        print(f\"Total Activities: {analysis_results['activity_stats']['total_activities']:,}\")\n",
    "    \n",
    "    if 'temporal_stats' in analysis_results:\n",
    "        print(f\"Time Period: {analysis_results['temporal_stats']['time_range'].days} days\")\n",
    "    \n",
    "    if performance_results and 'duration_stats' in performance_results:\n",
    "        print(f\"Avg Case Duration: {performance_results['duration_stats']['mean']:.1f} hours\")\n",
    "        print(f\"Median Duration: {performance_results['duration_stats']['50%']:.1f} hours\")\n",
    "    \n",
    "    print(\"\\nACTIONABLE RECOMMENDATIONS\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Based on variant analysis\n",
    "    if variant_results:\n",
    "        n_variants = len(variant_results['variant_counts'])\n",
    "        if n_variants > 100:\n",
    "            recommendations.append(\"High process variability detected. Consider standardizing process flows.\")\n",
    "    \n",
    "    # Based on performance analysis\n",
    "    if performance_results:\n",
    "        duration_std = performance_results['duration_stats']['std']\n",
    "        duration_mean = performance_results['duration_stats']['mean']\n",
    "        \n",
    "        if duration_std > duration_mean * 0.5:  # High variability\n",
    "            recommendations.append(\"High variability in case durations. Investigate root causes.\")\n",
    "        \n",
    "        # Check for bottlenecks\n",
    "        if performance_results.get('activity_wait_times') is not None:\n",
    "            top_bottleneck = performance_results['activity_wait_times'].iloc[0]\n",
    "            if top_bottleneck['mean'] > 24:  # More than 1 day wait\n",
    "                activity_name = performance_results['activity_wait_times'].index[0]\n",
    "                recommendations.append(f\"Major bottleneck at '{activity_name}' with {top_bottleneck['mean']:.1f}h average wait. Prioritize optimization.\")\n",
    "    \n",
    "    # Based on data quality\n",
    "    if 'resource_stats' in analysis_results:\n",
    "        resource_counts = analysis_results['resource_stats']['resource_counts']\n",
    "        if len(resource_counts) > 0:\n",
    "            busiest = resource_counts.iloc[0]\n",
    "            quietest = resource_counts.iloc[-1]\n",
    "            \n",
    "            if busiest > quietest * 10 and quietest > 0:\n",
    "                recommendations.append(\"Significant resource utilization imbalance detected. Consider workload balancing.\")\n",
    "    \n",
    "    # Based on temporal patterns\n",
    "    if '_dayofweek' in bpi_df.columns:\n",
    "        weekend_mask = bpi_df['_dayofweek'] >= 5\n",
    "        weekend_events = weekend_mask.sum()\n",
    "        \n",
    "        if weekend_events > len(bpi_df) * 0.05:  # More than 5% on weekends\n",
    "            recommendations.append(\"Significant weekend activity detected. Consider if 24/7 operations are needed or if processes can be optimized.\")\n",
    "    \n",
    "    # Print recommendations\n",
    "    if recommendations:\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"{i}. {rec}\")\n",
    "    else:\n",
    "        print(\"Process appears to be running efficiently. Maintain current operations.\")\n",
    "    \n",
    "    print(\"\\nSUGGESTED NEXT STEPS\")\n",
    "    print(\"-\"*40)\n",
    "    print(\"1. Implement real-time monitoring for cases exceeding duration thresholds\")\n",
    "    print(\"2. Create automated alerts for bottleneck activities\")\n",
    "    print(\"3. Develop dashboard showing: Case throughput, Resource utilization, Bottleneck status\")\n",
    "    print(\"4. Schedule regular process reviews based on these metrics\")\n",
    "    print(\"5. Consider process automation for high-frequency, repetitive activities\")\n",
    "    \n",
    "    print(\"\\nEXPORTING RESULTS\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    try:\n",
    "        # Export key findings to CSV\n",
    "        export_dir = base_path / \"BPI_Analysis_Results\"\n",
    "        export_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Export activity frequencies\n",
    "        if 'activity_stats' in analysis_results:\n",
    "            activity_df = analysis_results['activity_stats']['activity_counts'].reset_index()\n",
    "            activity_df.columns = ['Activity', 'Count']\n",
    "            activity_df['Percentage'] = (activity_df['Count'] / len(bpi_df)) * 100\n",
    "            activity_df.to_csv(export_dir / \"activity_frequencies.csv\", index=False)\n",
    "            print(f\"Exported activity frequencies\")\n",
    "        \n",
    "        # Export case duration statistics\n",
    "        if performance_results and 'case_times' in performance_results:\n",
    "            performance_results['case_times'].to_csv(export_dir / \"case_durations.csv\")\n",
    "            print(f\"Exported case durations\")\n",
    "        \n",
    "        # Export variant analysis\n",
    "        if variant_results:\n",
    "            variants_df = variant_results['variant_counts'].reset_index()\n",
    "            variants_df.columns = ['Variant_Path', 'Case_Count']\n",
    "            variants_df['Percentage'] = (variants_df['Case_Count'] / len(variant_results['traces'])) * 100\n",
    "            variants_df.to_csv(export_dir / \"process_variants.csv\", index=False)\n",
    "            print(f\"Exported process variants\")\n",
    "        \n",
    "        print(f\"\\nAll results exported to: {export_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting results: {str(e)}\")\n",
    "\n",
    "# Generate summary\n",
    "generate_summary_and_recommendations(analysis_results, variant_results, performance_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"Successfully completed comprehensive BPI 2019 analysis\")\n",
    "print(\"Analyzed data with proper handling of categorical ordering\")\n",
    "print(\"Generated actionable insights and recommendations\")\n",
    "print(\"Results available for further analysis in Tableau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1261c0da-777b-4f56-a26e-60f95004bd29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
